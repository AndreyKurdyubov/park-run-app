import os
import streamlit as st
import requests
from concurrent.futures import ThreadPoolExecutor
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, text
from datetime import datetime
import pandas as pd  
import aiohttp
import asyncio

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title='PARKüå≥RUN', page_icon=':running:')

# –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
image_path = 'logo.jpg'  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é

# –í—Å—Ç–∞–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
st.image(image_path, caption='', width=250)

# –°–∫—Ä—ã—Ç–∏–µ —Ñ—É—Ç–µ—Ä–∞ –∏ –º–µ–Ω—é
hide_streamlit_style = """
            <style>
            MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            </style>
            """
            
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
st.title('–î–æ–º–∞—à–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞')

st.subheader('–û–ø–∏—Å–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –∏ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü')

st.markdown('''
–¢—É—Ç –≤–æ–∑–º–æ–∂–Ω–æ –±—É–¥–µ—Ç –∫–∞–∫–æ–µ-—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.  
–°–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü:
- –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
- –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∫–æ—Ä–¥–æ–≤
- –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å —Ç–∞–±–ª–∏—Ü–µ–π –±–µ–≥—É–Ω–æ–≤
- –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å —Ç–∞–±–ª–∏—Ü–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
''')

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–∞—Ç—ã –∏–∑ —Å–∞–π—Ç–∞
def get_last_date_from_site():
    url = 'https://5verst.ru/petergofaleksandriysky/results/all/'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    last_date = soup.find_all('table')[0].find_all('tr')[1].find_all('td')[1].text.strip()

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ last_date –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ DD.MM.YYYY –≤ –æ–±—ä–µ–∫—Ç datetime
    last_date_site = datetime.strptime(last_date, '%d.%m.%Y').date()
    return last_date_site

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–∞—Ç—ã –∏–∑ –ë–î
def get_last_date_from_db(db_url='sqlite:///mydatabase.db'):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    db_path = db_url.replace('sqlite:///', '')  # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists(db_path):
        return None  # –ï—Å–ª–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    engine = create_engine(db_url)
    with engine.connect() as connection:
        query = text("SELECT MAX(run_date) FROM runners")  # –ó–∞–º–µ–Ω–∏—Ç—å run_date –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–µ –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–æ–π
        result = connection.execute(query)
        last_date_db = result.scalar()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ last_date_db –Ω–µ None, —Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ –¥–∞—Ç—É
        if last_date_db:
            last_date_db = datetime.strptime(last_date_db, '%Y-%m-%d %H:%M:%S.%f').date()
        else:
            last_date_db = None
    return last_date_db

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –ø–æ –∑–∞–±–µ–≥–∞–º –ª–æ–∫–∞—Ü–∏–∏
def parse_run_page(run_link):
    try:
        response = requests.get(run_link)
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        run_table = soup.find('table')

        run_data = []
        if run_table:
            for run_row in run_table.find_all('tr')[1:]:
                run_cells = run_row.find_all('td')
                if len(run_cells) >= 4:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–∞–∂–¥–æ–º—É –∑–∞–±–µ–≥—É
                    number = run_cells[0].get_text(strip=True)
                    date_cell = run_cells[1].get_text(strip=True)
                    link = run_cells[1].find('a')['href'] if run_cells[1].find('a') else None
                    finishers = run_cells[2].get_text(strip=True)
                    volunteers = run_cells[3].get_text(strip=True)
                    avg_time = run_cells[4].get_text(strip=True)
                    best_female_time = run_cells[5].get_text(strip=True)
                    best_male_time = run_cells[6].get_text(strip=True)
                    run_data.append([number, date_cell, link, finishers, volunteers, avg_time, best_female_time, best_male_time])
        return run_data
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {run_link}: {e}")
        return None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–∞–±–ª–∏—Ü —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ –≤–æ–ª–æ–Ω—Ç—ë—Ä–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–±–µ–≥–∞
def parse_participant_and_volunteer_tables(run_link):
    try:
        response = requests.get(run_link)
        if response.status_code != 200:
            return None, None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        all_tables = soup.find_all('table')

        if len(all_tables) < 2:
            return None, None  # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–∞–±–ª–∏—Ü

        # –ü–µ—Ä–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ ‚Äì —É—á–∞—Å—Ç–Ω–∏–∫–∏
        participants_data = []
        participant_table = all_tables[0]
        for row in participant_table.find_all('tr')[1:]:
            cells = row.find_all('td')
            if len(cells) >= 4:
                position = cells[0].get_text(strip=True)
                name_tag = cells[1].find('a')
                name = name_tag.get_text(strip=True) if name_tag else '‚Äî'
                profile_link = name_tag['href'] if name_tag else '‚Äî'
                participant_id = profile_link.split('/')[-1] if profile_link != '‚Äî' else '‚Äî'
                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–Ω–∏—à–µ–π –∏ –≤–æ–ª–æ–Ω—Ç—ë—Ä—Å—Ç–≤
                stats_div = cells[1].find('div', class_='user-stat')
                finishes = '‚Äî'
                volunteers = '‚Äî'
                if stats_div:
                    stats_spans = stats_div.find_all('span')
                    finishes = stats_spans[0].get_text(strip=True).split(' ')[0] if len(stats_spans) > 0 else '‚Äî'
                    volunteers = stats_spans[1].get_text(strip=True).split(' ')[0] if len(stats_spans) > 1 else '‚Äî'

                # –ö–ª—É–±—ã
                club_tags = cells[1].find_all('span', class_='club-icon')
                clubs = ', '.join([club['title'] for club in club_tags]) if club_tags else '‚Äî'

                # –í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞ –∏ Age Grade
                age_group = cells[2].get_text(strip=True).split(' ')[0] if cells[2] else '‚Äî'
                age_grade_tag = cells[2].find('div', class_='age_grade')
                age_grade = age_grade_tag.get_text(strip=True) if age_grade_tag else '‚Äî'

                # –í—Ä–µ–º—è –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è
                time = cells[3].get_text(strip=True) if cells[3] else '‚Äî'
                achievements = []
                achievements_div = cells[3].find('div', class_='table-achievments')
                if achievements_div:
                    achievement_icons = achievements_div.find_all('span', class_='results_icon')
                    for icon in achievement_icons:
                        achievements.append(icon['title'])  # –û–ø–∏—Å–∞–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è
                participants_data.append([position, name, profile_link, participant_id, clubs, finishes, volunteers, age_group, age_grade, time, ', '.join(achievements)])

        # –í—Ç–æ—Ä–∞—è —Ç–∞–±–ª–∏—Ü–∞ ‚Äì –≤–æ–ª–æ–Ω—Ç—ë—Ä—ã
        volunteers_data = []
        volunteer_table = all_tables[1]
        for row in volunteer_table.find_all('tr')[1:]:
            columns = row.find_all('td')
            if len(columns) > 1:
                name_tag = columns[0].find('a')
                name = name_tag.get_text(strip=True) if name_tag else '‚Äî'
                profile_link = name_tag['href'] if name_tag else '‚Äî'
                participant_id = profile_link.split('/')[-1] if profile_link != '‚Äî' else '‚Äî'
                                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–Ω–∏—à–µ–π –∏ –≤–æ–ª–æ–Ω—Ç—ë—Ä—Å—Ç–≤
                stats_div = columns[0].find('div', class_='user-stat')
                finishes = '‚Äî'
                volunteers = '‚Äî'
                if stats_div:
                    stats_spans = stats_div.find_all('span')
                    finishes = stats_spans[0].get_text(strip=True).split(' ')[0] if len(stats_spans) > 0 else '‚Äî'
                    volunteers = stats_spans[1].get_text(strip=True).split(' ')[0] if len(stats_spans) > 1 else '‚Äî'

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—É–±—ã (–≤—Å–µ –∫–ª—É–±—ã)
                club_tags = columns[0].find_all('span', class_='club-icon')
                clubs = ', '.join([club['title'] for club in club_tags]) if club_tags else '‚Äî'

                # –í—Ç–æ—Ä–∞—è –∫–æ–ª–æ–Ω–∫–∞: —Ä–æ–ª—å –≤–æ–ª–æ–Ω—Ç—ë—Ä–∞ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–µ—Ä–≤–æ–º –≤–æ–ª–æ–Ω—Ç—ë—Ä—Å—Ç–≤–µ
                volunteer_role_info = columns[1].find('div', class_='volunteer__role')
                if volunteer_role_info:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç title –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–µ—Ä–≤–æ–º –≤–æ–ª–æ–Ω—Ç—ë—Ä—Å—Ç–≤–µ
                    first_volunteer_tag = volunteer_role_info.find('span', class_='results_icon')
                    first_volunteer_info = first_volunteer_tag['title'] if first_volunteer_tag else '‚Äî'

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Ä–æ–ª–∏ –≤–æ–ª–æ–Ω—Ç—ë—Ä–∞
                    role_tag = volunteer_role_info.find_all('span')
                    volunteer_role = role_tag[-1].get_text(strip=True) if role_tag else '‚Äî'
                else:
                    first_volunteer_info = '‚Äî'
                    volunteer_role = '‚Äî'
                volunteers_data.append([name, profile_link, participant_id, finishes, volunteers, clubs, volunteer_role, first_volunteer_info])

        return participants_data, volunteers_data
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ç–∞–±–ª–∏—Ü —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ –≤–æ–ª–æ–Ω—Ç—ë—Ä–æ–≤: {e}")
        return None, None

# –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–±–µ–≥–æ–≤ –∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
def process_run_data(filtered_starts_latest):
    orgs_data = []
    runners_data = []

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(parse_run_page, row['run_link']) for _, row in filtered_starts_latest.iterrows()]

        for future, row in zip(futures, filtered_starts_latest.itertuples()):
            run_data = future.result()
            if run_data:
                for item in run_data:
                    # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–±–µ–≥–∞
                    run_link = item[2]
                    participants, volunteers = parse_participant_and_volunteer_tables(run_link)

                    if participants:
                        for p in participants:
                            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —É—á–∞—Å—Ç–Ω–∏–∫–∞
                            runners_data.append([
                                row.run, item[0], item[1], run_link, item[3], item[4], item[5], item[6], item[7]
                            ] + p)

                    if volunteers:
                        for v in volunteers:
                            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤–æ–ª–æ–Ω—Ç—ë—Ä–∞
                            orgs_data.append([
                                row.run, item[0], item[1], run_link, item[3], item[4], item[5], item[6], item[7]
                            ] + v)

    return orgs_data, runners_data

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã –∑–∞–±–µ–≥–æ–≤
def parse_main_table(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # –ù–∞—Ö–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    table = soup.find('table')

    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    starts_latest = []
    for row in table.find_all('tr')[1:]:
        cells = row.find_all('td')
        if len(cells) > 0:
            number_cell = cells[0]
            run = number_cell.text.strip().split(' #')[0]  # –°—Ç–∞—Ä—Ç
            link = number_cell.find('a')['href'] if number_cell.find('a') else None
            date = cells[1].text.strip()  # –î–∞—Ç–∞ –∑–∞–±–µ–≥–∞
            finishers = cells[2].text.strip()  # –ß–∏—Å–ª–æ —Ñ–∏–Ω–∏—à—ë—Ä–æ–≤
            volunteers = cells[3].text.strip()  # –ß–∏—Å–ª–æ –≤–æ–ª–æ–Ω—Ç—ë—Ä–æ–≤
            avg_time = cells[4].text.strip()  # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è
            best_female_time = cells[5].text.strip()  # –õ—É—á—à–µ–µ –≤—Ä–µ–º—è "–ñ"
            best_male_time = cells[6].text.strip()  # –õ—É—á—à–µ–µ –≤—Ä–µ–º—è "–ú"
            starts_latest.append([run, date, link, finishers, volunteers, avg_time, best_female_time, best_male_time])

    return pd.DataFrame(starts_latest, columns=[
        'run', 'run_date', 'run_link', 'finishers', 'volunteers', 'avg_time', 'best_female_time', 'best_male_time'
    ])

# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞
def parse_website():
    url = 'https://5verst.ru/results/latest/'
    
    # –ü–∞—Ä—Å–∏–º –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –∑–∞–±–µ–≥–∞–º–∏
    starts_latest_df = parse_main_table(url)

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–±–µ–≥–æ–≤ –ø–æ –Ω—É–∂–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏—è–º
    target_runs = ['–ü–µ—Ç–µ—Ä–≥–æ—Ñ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∏–π—Å–∫–∏–π']  # –ó–∞–º–µ–Ω–∏—Ç–µ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    filtered_starts_latest = starts_latest_df[starts_latest_df['run'].isin(target_runs)]

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–±–µ–≥–æ–≤ –∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
    orgs_data, runners_data = process_run_data(filtered_starts_latest)
    
    return orgs_data, runners_data

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
def save_to_database(df_orgs, df_runners, df_stats, db_url='sqlite:///mydatabase.db'):
    # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    engine = create_engine(db_url)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü—É 'organizers'
    df_orgs.to_sql('organizers', con=engine, if_exists='replace', index=False)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –±–µ–≥—É–Ω–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü—É 'runners'
    df_runners.to_sql('runners', con=engine, if_exists='replace', index=False)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –±–µ–≥—É–Ω–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü—É 'runners'
    df_stats.to_sql('users', con=engine, if_exists='replace', index=False)

async def fetch_profile_data(session, url, df_runners):
    async with session.get(url) as response:
        html = await response.text()
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ
        row = df_runners[df_runners['profile_link'] == url].iloc[0]  
        first_name = row['name'].split()[0]
        last_name = row['name'].split()[1]
        participant_id = row['participant_id']
        profile_link = row['profile_link']

        stats_div = soup.find('div', class_='grid grid-cols-2 gap-px bg-black/[0.05]')
        finishes = stats_div.find_all('div', class_='bg-white p-4')[0].find('span', class_='text-3xl font-semibold tracking-tight').text.strip()
        volunteers = stats_div.find_all('div', class_='bg-white p-4')[1].find('span', class_='text-3xl font-semibold tracking-tight').text.strip()
        best_time = stats_div.find_all('div', class_='bg-white p-4')[2].find('span', class_='text-3xl font-semibold tracking-tight').text.strip()
        best_time_link = stats_div.find_all('div', class_='bg-white p-4')[2].find('a', class_='user-info-park-link')['href']

        clubs = stats_div.find_all('div', class_='bg-white p-4')[3].find_all('span', class_='club-icon')
        clubs_titles = ', '.join([club['title'] for club in clubs])

        tables = soup.find_all('table')
        peterhof_finishes_count = sum(1 for row in tables[0].find_all('tr')[1:] if '–ü–µ—Ç–µ—Ä–≥–æ—Ñ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∏–π—Å–∫–∏–π' in row.find_all('td')[1].text.strip())
        peterhof_volunteers_count = sum(1 for row in tables[1].find_all('tr')[1:] if '–ü–µ—Ç–µ—Ä–≥–æ—Ñ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∏–π—Å–∫–∏–π' in row.find_all('td')[1].text.strip())
        
        return [participant_id, profile_link, first_name, last_name, best_time, finishes, 
                peterhof_finishes_count, volunteers, peterhof_volunteers_count, clubs_titles, best_time_link]

async def gather_profiles_data(urls, df_runners):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for url in urls:
            if 'https://5verst.ru/userstats/' in url:
                task = fetch_profile_data(session, url, df_runners)
                tasks.append(task)
        return await asyncio.gather(*tasks)

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞
def run_parsing_async():
    orgs_data, runners_data = parse_website()

    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
    df_orgs = pd.DataFrame(orgs_data, columns=[
        'run', 'run_number', 'run_date', 'run_link', 'finisher', 'volunteer', 'avg_time', 'best_female_time', 'best_male_time', 
        'name', 'profile_link', 'participant_id', 'finishes', 'volunteers', 'clubs', 'volunteer_role', 'first_volunteer_info'
    ])
    df_orgs['run_date'] = pd.to_datetime(df_orgs['run_date'], dayfirst=True)
    df_orgs['finisher'] = df_orgs['finisher'].astype('int')
    df_orgs['volunteer'] = df_orgs['volunteer'].astype('int')

    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –±–µ–≥—É–Ω–æ–≤
    df_runners = pd.DataFrame(runners_data, columns=[
        'run', 'run_number', 'run_date', 'run_link', 'finisher', 'volunteer', 'avg_time', 'best_female_time', 'best_male_time', 
        'position', 'name', 'profile_link', 'participant_id', 'clubs', 'finishes', 'volunteers', 'age_group', 'age_grade', 'time', 'achievements'
    ])
    df_runners['run_date'] = pd.to_datetime(df_runners['run_date'], dayfirst=True)
    df_runners['finisher'] = df_runners['finisher'].astype('int')
    df_runners['volunteer'] = df_runners['volunteer'].astype('int')

    urls = df_runners['profile_link'].unique()

    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    stats_data = asyncio.run(gather_profiles_data(urls, df_runners))

    df_stats = pd.DataFrame(stats_data, columns=[
        'participant_id', 'profile_link', 'first_name', 'last_name', 'best_time', 'finishes', 
        'peterhof_finishes_count', 'volunteers', 'peterhof_volunteers_count', 'clubs', 'best_time_link'
    ])

    save_to_database(df_orgs, df_runners, df_stats)


last_date_site = get_last_date_from_site()
last_date_db = get_last_date_from_db()
    
st.subheader('–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö')  
# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ last_date_db –Ω–µ –ø—É—Å—Ç–∞—è
if last_date_db is None:
    st.write('–î–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ –Ω–µ—Ç!')
    if st.button('–û–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ'):
        st.write('–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö...')
        run_parsing_async()
        st.success('–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!')
else:
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞—Ç
    if last_date_db != last_date_site:
        st.write(f'–î–∞–Ω–Ω—ã–µ —É—Å—Ç–∞—Ä–µ–ª–∏. –î–∞—Ç–∞ –≤ –±–∞–∑–µ: {last_date_db}, –¥–∞—Ç–∞ –Ω–∞ —Å–∞–π—Ç–µ: {last_date_site}.')
        
        if st.button('–û–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ'):
            st.write('–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö...')
            run_parsing_async()
            st.success('–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!')
    else:
        st.markdown(f'''–î–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã üëç  
                    –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {last_date_db}  
                    –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –Ω–∞ —Å–∞–π—Ç–µ: {last_date_site}
                    ''')

st.subheader('–ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏')        
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
def search_database(query):
    db_url = 'sqlite:///mydatabase.db'
    engine = create_engine(db_url)
    with engine.connect() as connection:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SQL –¥–ª—è –ø–æ–∏—Å–∫–∞
        query_like = f"%{query}%"
        sql_query = text("SELECT * FROM runners WHERE name LIKE :query")
        result = connection.execute(sql_query, {"query": query_like}).fetchall()  # –ü–µ—Ä–µ–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å
    return result

# –ü–æ–ª–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
search_query = st.text_input("–ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏ –±–µ–≥—É–Ω–∞:")

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
if search_query:
    search_results = search_database(search_query)

    if search_results:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
        df_results = pd.DataFrame(search_results)  # –ë–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫
        st.dataframe(df_results)  # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã
    else:
        st.write("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.")